{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backpropagation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN89ftNW55PmCBenHDs6B3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghosesuvendu/dataScience/blob/main/backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtyTir4rWWdu",
        "outputId": "e6428946-d915-4791-88ed-5ad9abe82771"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        " \n",
        "# Find a small float to avoid division by zero\n",
        "epsilon = np.finfo(float).eps\n",
        " \n",
        "# Sigmoid function and its differentiation\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z.clip(-500, 500)))\n",
        "def dsigmoid(z):\n",
        "    s = sigmoid(z)\n",
        "    return 2 * s * (1-s)\n",
        " \n",
        "# ReLU function and its differentiation\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "def drelu(z):\n",
        "    return (z > 0).astype(float)\n",
        " \n",
        "# Loss function L(y, yhat) and its differentiation\n",
        "def cross_entropy(y, yhat):\n",
        "    \"\"\"Binary cross entropy function\n",
        "        L = - y log yhat - (1-y) log (1-yhat)\n",
        " \n",
        "    Args:\n",
        "        y, yhat (np.array): nx1 matrices which n are the number of data instances\n",
        "    Returns:\n",
        "        average cross entropy value of shape 1x1, averaging over the n instances\n",
        "    \"\"\"\n",
        "    return -(y.T @ np.log(yhat.clip(epsilon)) + (1-y.T) @ np.log((1-yhat).clip(epsilon))) / y.shape[1]\n",
        " \n",
        "def d_cross_entropy(y, yhat):\n",
        "    \"\"\" dL/dyhat \"\"\"\n",
        "    return - np.divide(y, yhat.clip(epsilon)) + np.divide(1-y, (1-yhat).clip(epsilon))\n",
        " \n",
        "class mlp:\n",
        "    '''Multilayer perceptron using numpy\n",
        "    '''\n",
        "    def __init__(self, layersizes, activations, derivatives, lossderiv):\n",
        "        \"\"\"remember config, then initialize array to hold NN parameters without init\"\"\"\n",
        "        # hold NN config\n",
        "        self.layersizes = tuple(layersizes)\n",
        "        self.activations = tuple(activations)\n",
        "        self.derivatives = tuple(derivatives)\n",
        "        self.lossderiv = lossderiv\n",
        "        assert len(self.layersizes)-1 == len(self.activations), \\\n",
        "            \"number of layers and the number of activation functions does not match\"\n",
        "        assert len(self.activations) == len(self.derivatives), \\\n",
        "            \"number of activation functions and number of derivatives does not match\"\n",
        "        assert all(isinstance(n, int) and n >= 1 for n in layersizes), \\\n",
        "            \"Only positive integral number of perceptons is allowed in each layer\"\n",
        "        # parameters, each is a 2D numpy array\n",
        "        L = len(self.layersizes)\n",
        "        self.z = [None] * L\n",
        "        self.W = [None] * L\n",
        "        self.b = [None] * L\n",
        "        self.a = [None] * L\n",
        "        self.dz = [None] * L\n",
        "        self.dW = [None] * L\n",
        "        self.db = [None] * L\n",
        "        self.da = [None] * L\n",
        " \n",
        "    def initialize(self, seed=42):\n",
        "        \"\"\"initialize the value of weight matrices and bias vectors with small random numbers.\"\"\"\n",
        "        np.random.seed(seed)\n",
        "        sigma = 0.1\n",
        "        for l, (insize, outsize) in enumerate(zip(self.layersizes, self.layersizes[1:]), 1):\n",
        "            self.W[l] = np.random.randn(insize, outsize) * sigma\n",
        "            self.b[l] = np.random.randn(1, outsize) * sigma\n",
        " \n",
        "    def forward(self, x):\n",
        "        \"\"\"Feed forward using existing `W` and `b`, and overwrite the result variables `a` and `z`\n",
        " \n",
        "        Args:\n",
        "            x (numpy.ndarray): Input data to feed forward\n",
        "        \"\"\"\n",
        "        self.a[0] = x\n",
        "        for l, func in enumerate(self.activations, 1):\n",
        "            # z = W a + b, with `a` as output from previous layer\n",
        "            # `W` is of size rxs and `a` the size sxn with n the number of data instances, `z` the size rxn\n",
        "            # `b` is rx1 and broadcast to each column of `z`\n",
        "            self.z[l] = (self.a[l-1] @ self.W[l]) + self.b[l]\n",
        "            # a = g(z), with `a` as output of this layer, of size rxn\n",
        "            self.a[l] = func(self.z[l])\n",
        "        return self.a[-1]\n",
        " \n",
        "    def backward(self, y, yhat):\n",
        "        \"\"\"back propagation using NN output yhat and the reference output y, generates dW, dz, db,\n",
        "        da\n",
        "        \"\"\"\n",
        "        assert y.shape[1] == self.layersizes[-1], \"Output size doesn't match network output size\"\n",
        "        assert y.shape == yhat.shape, \"Output size doesn't match reference\"\n",
        "        # first `da`, at the output\n",
        "        self.da[-1] = self.lossderiv(y, yhat)\n",
        "        for l, func in reversed(list(enumerate(self.derivatives, 1))):\n",
        "            # compute the differentials at this layer\n",
        "            self.dz[l] = self.da[l] * func(self.z[l])\n",
        "            self.dW[l] = self.a[l-1].T @ self.dz[l]\n",
        "            self.db[l] = np.mean(self.dz[l], axis=0, keepdims=True)\n",
        "            self.da[l-1] = self.dz[l] @ self.W[l].T\n",
        "            assert self.z[l].shape == self.dz[l].shape\n",
        "            assert self.W[l].shape == self.dW[l].shape\n",
        "            assert self.b[l].shape == self.db[l].shape\n",
        "            assert self.a[l].shape == self.da[l].shape\n",
        " \n",
        "    def update(self, eta):\n",
        "        \"\"\"Updates W and b\n",
        " \n",
        "        Args:\n",
        "            eta (float): Learning rate\n",
        "        \"\"\"\n",
        "        for l in range(1, len(self.W)):\n",
        "            self.W[l] -= eta * self.dW[l]\n",
        "            self.b[l] -= eta * self.db[l]\n",
        " \n",
        "# Make data: Two circles on x-y plane as a classification problem\n",
        "X, y = make_circles(n_samples=1000, factor=0.5, noise=0.1)\n",
        "y = y.reshape(-1,1) # our model expects a 2D array of (n_sample, n_dim)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        " \n",
        "# Build a model\n",
        "model = mlp(layersizes=[2, 4, 3, 1],\n",
        "            activations=[relu, relu, sigmoid],\n",
        "            derivatives=[drelu, drelu, dsigmoid],\n",
        "            lossderiv=d_cross_entropy)\n",
        "model.initialize()\n",
        "yhat = model.forward(X)\n",
        "loss = cross_entropy(y, yhat)\n",
        "print(\"Before training - loss value {} accuracy {}\".format(loss, accuracy_score(y, (yhat > 0.5))))\n",
        " \n",
        "# train for each epoch\n",
        "n_epochs = 150\n",
        "learning_rate = 0.005\n",
        "for n in range(n_epochs):\n",
        "    model.forward(X)\n",
        "    yhat = model.a[-1]\n",
        "    model.backward(y, yhat)\n",
        "    model.update(learning_rate)\n",
        "    loss = cross_entropy(y, yhat)\n",
        "    print(\"Iteration {} - loss value {} accuracy {}\".format(n, loss, accuracy_score(y, (yhat > 0.5))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n",
            "(1000, 1)\n",
            "Before training - loss value [[693.62972747]] accuracy 0.5\n",
            "Iteration 0 - loss value [[693.62972747]] accuracy 0.5\n",
            "Iteration 1 - loss value [[693.62166655]] accuracy 0.5\n",
            "Iteration 2 - loss value [[693.61534159]] accuracy 0.5\n",
            "Iteration 3 - loss value [[693.60994018]] accuracy 0.5\n",
            "Iteration 4 - loss value [[693.60515795]] accuracy 0.5\n",
            "Iteration 5 - loss value [[693.60082044]] accuracy 0.5\n",
            "Iteration 6 - loss value [[693.59683747]] accuracy 0.5\n",
            "Iteration 7 - loss value [[693.59314676]] accuracy 0.5\n",
            "Iteration 8 - loss value [[693.5896873]] accuracy 0.5\n",
            "Iteration 9 - loss value [[693.58643119]] accuracy 0.5\n",
            "Iteration 10 - loss value [[693.58333162]] accuracy 0.5\n",
            "Iteration 11 - loss value [[693.58036603]] accuracy 0.5\n",
            "Iteration 12 - loss value [[693.57752348]] accuracy 0.5\n",
            "Iteration 13 - loss value [[693.57478667]] accuracy 0.5\n",
            "Iteration 14 - loss value [[693.57214202]] accuracy 0.5\n",
            "Iteration 15 - loss value [[693.56957462]] accuracy 0.5\n",
            "Iteration 16 - loss value [[693.56707586]] accuracy 0.5\n",
            "Iteration 17 - loss value [[693.56464362]] accuracy 0.5\n",
            "Iteration 18 - loss value [[693.56227014]] accuracy 0.5\n",
            "Iteration 19 - loss value [[693.55994246]] accuracy 0.5\n",
            "Iteration 20 - loss value [[693.5576544]] accuracy 0.5\n",
            "Iteration 21 - loss value [[693.55540145]] accuracy 0.5\n",
            "Iteration 22 - loss value [[693.55317958]] accuracy 0.5\n",
            "Iteration 23 - loss value [[693.55098478]] accuracy 0.5\n",
            "Iteration 24 - loss value [[693.54881329]] accuracy 0.5\n",
            "Iteration 25 - loss value [[693.54666177]] accuracy 0.5\n",
            "Iteration 26 - loss value [[693.5445273]] accuracy 0.5\n",
            "Iteration 27 - loss value [[693.54240705]] accuracy 0.5\n",
            "Iteration 28 - loss value [[693.54029831]] accuracy 0.5\n",
            "Iteration 29 - loss value [[693.53819842]] accuracy 0.5\n",
            "Iteration 30 - loss value [[693.53610471]] accuracy 0.5\n",
            "Iteration 31 - loss value [[693.53401451]] accuracy 0.5\n",
            "Iteration 32 - loss value [[693.53192505]] accuracy 0.5\n",
            "Iteration 33 - loss value [[693.52983341]] accuracy 0.5\n",
            "Iteration 34 - loss value [[693.52773618]] accuracy 0.5\n",
            "Iteration 35 - loss value [[693.5256298]] accuracy 0.5\n",
            "Iteration 36 - loss value [[693.52351051]] accuracy 0.5\n",
            "Iteration 37 - loss value [[693.52137475]] accuracy 0.5\n",
            "Iteration 38 - loss value [[693.51921787]] accuracy 0.5\n",
            "Iteration 39 - loss value [[693.51703291]] accuracy 0.5\n",
            "Iteration 40 - loss value [[693.51480672]] accuracy 0.5\n",
            "Iteration 41 - loss value [[693.51253633]] accuracy 0.5\n",
            "Iteration 42 - loss value [[693.51021849]] accuracy 0.5\n",
            "Iteration 43 - loss value [[693.50784395]] accuracy 0.5\n",
            "Iteration 44 - loss value [[693.50540015]] accuracy 0.5\n",
            "Iteration 45 - loss value [[693.50287628]] accuracy 0.5\n",
            "Iteration 46 - loss value [[693.50025782]] accuracy 0.5\n",
            "Iteration 47 - loss value [[693.49753386]] accuracy 0.5\n",
            "Iteration 48 - loss value [[693.4946915]] accuracy 0.5\n",
            "Iteration 49 - loss value [[693.49168574]] accuracy 0.5\n",
            "Iteration 50 - loss value [[693.48849362]] accuracy 0.5\n",
            "Iteration 51 - loss value [[693.48506738]] accuracy 0.5\n",
            "Iteration 52 - loss value [[693.48136697]] accuracy 0.5\n",
            "Iteration 53 - loss value [[693.47734281]] accuracy 0.5\n",
            "Iteration 54 - loss value [[693.47290345]] accuracy 0.5\n",
            "Iteration 55 - loss value [[693.46790337]] accuracy 0.5\n",
            "Iteration 56 - loss value [[693.46218985]] accuracy 0.5\n",
            "Iteration 57 - loss value [[693.45554275]] accuracy 0.5\n",
            "Iteration 58 - loss value [[693.44730115]] accuracy 0.5\n",
            "Iteration 59 - loss value [[693.43668907]] accuracy 0.5\n",
            "Iteration 60 - loss value [[693.42344913]] accuracy 0.5\n",
            "Iteration 61 - loss value [[693.40707519]] accuracy 0.5\n",
            "Iteration 62 - loss value [[693.38932516]] accuracy 0.5\n",
            "Iteration 63 - loss value [[693.3711731]] accuracy 0.5\n",
            "Iteration 64 - loss value [[693.35194155]] accuracy 0.5\n",
            "Iteration 65 - loss value [[693.33133414]] accuracy 0.5\n",
            "Iteration 66 - loss value [[693.30857931]] accuracy 0.5\n",
            "Iteration 67 - loss value [[693.2833329]] accuracy 0.5\n",
            "Iteration 68 - loss value [[693.25547887]] accuracy 0.5\n",
            "Iteration 69 - loss value [[693.22471545]] accuracy 0.5\n",
            "Iteration 70 - loss value [[693.19076905]] accuracy 0.5\n",
            "Iteration 71 - loss value [[693.15359149]] accuracy 0.5\n",
            "Iteration 72 - loss value [[693.11307793]] accuracy 0.5\n",
            "Iteration 73 - loss value [[693.06929743]] accuracy 0.5\n",
            "Iteration 74 - loss value [[693.02279656]] accuracy 0.5\n",
            "Iteration 75 - loss value [[692.97352903]] accuracy 0.5\n",
            "Iteration 76 - loss value [[692.92114923]] accuracy 0.5\n",
            "Iteration 77 - loss value [[692.86540748]] accuracy 0.5\n",
            "Iteration 78 - loss value [[692.80581574]] accuracy 0.5\n",
            "Iteration 79 - loss value [[692.74149305]] accuracy 0.5\n",
            "Iteration 80 - loss value [[692.67261138]] accuracy 0.5\n",
            "Iteration 81 - loss value [[692.59815315]] accuracy 0.5\n",
            "Iteration 82 - loss value [[692.51755087]] accuracy 0.5\n",
            "Iteration 83 - loss value [[692.43089381]] accuracy 0.5\n",
            "Iteration 84 - loss value [[692.34064307]] accuracy 0.5\n",
            "Iteration 85 - loss value [[692.26736399]] accuracy 0.5\n",
            "Iteration 86 - loss value [[692.25726955]] accuracy 0.5\n",
            "Iteration 87 - loss value [[692.06877948]] accuracy 0.5\n",
            "Iteration 88 - loss value [[691.92184714]] accuracy 0.5\n",
            "Iteration 89 - loss value [[691.83522319]] accuracy 0.5\n",
            "Iteration 90 - loss value [[691.70951762]] accuracy 0.5\n",
            "Iteration 91 - loss value [[691.74840714]] accuracy 0.5\n",
            "Iteration 92 - loss value [[691.35431923]] accuracy 0.5\n",
            "Iteration 93 - loss value [[691.14605227]] accuracy 0.5\n",
            "Iteration 94 - loss value [[691.23258499]] accuracy 0.5\n",
            "Iteration 95 - loss value [[691.06656704]] accuracy 0.5\n",
            "Iteration 96 - loss value [[691.36733012]] accuracy 0.5\n",
            "Iteration 97 - loss value [[691.13460669]] accuracy 0.5\n",
            "Iteration 98 - loss value [[690.2979817]] accuracy 0.5\n",
            "Iteration 99 - loss value [[691.02916437]] accuracy 0.5\n",
            "Iteration 100 - loss value [[691.14411224]] accuracy 0.5\n",
            "Iteration 101 - loss value [[690.68907032]] accuracy 0.5\n",
            "Iteration 102 - loss value [[689.48488382]] accuracy 0.5\n",
            "Iteration 103 - loss value [[690.35457369]] accuracy 0.5\n",
            "Iteration 104 - loss value [[690.56241278]] accuracy 0.466\n",
            "Iteration 105 - loss value [[690.00368902]] accuracy 0.5\n",
            "Iteration 106 - loss value [[688.84378943]] accuracy 0.5\n",
            "Iteration 107 - loss value [[690.04753338]] accuracy 0.5\n",
            "Iteration 108 - loss value [[688.09993779]] accuracy 0.5\n",
            "Iteration 109 - loss value [[689.24260746]] accuracy 0.5\n",
            "Iteration 110 - loss value [[687.89519168]] accuracy 0.709\n",
            "Iteration 111 - loss value [[688.33872228]] accuracy 0.508\n",
            "Iteration 112 - loss value [[687.84147107]] accuracy 0.875\n",
            "Iteration 113 - loss value [[688.39173321]] accuracy 0.525\n",
            "Iteration 114 - loss value [[686.27309392]] accuracy 0.837\n",
            "Iteration 115 - loss value [[685.04667475]] accuracy 0.62\n",
            "Iteration 116 - loss value [[689.11881189]] accuracy 0.636\n",
            "Iteration 117 - loss value [[683.88323248]] accuracy 0.697\n",
            "Iteration 118 - loss value [[684.13138046]] accuracy 0.654\n",
            "Iteration 119 - loss value [[682.69805027]] accuracy 0.718\n",
            "Iteration 120 - loss value [[687.31445055]] accuracy 0.5\n",
            "Iteration 121 - loss value [[675.34162754]] accuracy 0.922\n",
            "Iteration 122 - loss value [[715.05866237]] accuracy 0.565\n",
            "Iteration 123 - loss value [[688.96748754]] accuracy 0.555\n",
            "Iteration 124 - loss value [[680.85279539]] accuracy 0.75\n",
            "Iteration 125 - loss value [[685.29061931]] accuracy 0.692\n",
            "Iteration 126 - loss value [[681.63277681]] accuracy 0.724\n",
            "Iteration 127 - loss value [[682.3873837]] accuracy 0.645\n",
            "Iteration 128 - loss value [[680.75980948]] accuracy 0.724\n",
            "Iteration 129 - loss value [[680.73191417]] accuracy 0.748\n",
            "Iteration 130 - loss value [[684.18768312]] accuracy 0.674\n",
            "Iteration 131 - loss value [[680.64438775]] accuracy 0.741\n",
            "Iteration 132 - loss value [[678.08350269]] accuracy 0.764\n",
            "Iteration 133 - loss value [[681.61475823]] accuracy 0.676\n",
            "Iteration 134 - loss value [[679.97537818]] accuracy 0.72\n",
            "Iteration 135 - loss value [[680.8380265]] accuracy 0.723\n",
            "Iteration 136 - loss value [[674.17258804]] accuracy 0.63\n",
            "Iteration 137 - loss value [[687.39820594]] accuracy 0.604\n",
            "Iteration 138 - loss value [[677.66270218]] accuracy 0.731\n",
            "Iteration 139 - loss value [[672.67301808]] accuracy 0.735\n",
            "Iteration 140 - loss value [[688.4429775]] accuracy 0.676\n",
            "Iteration 141 - loss value [[686.58844806]] accuracy 0.556\n",
            "Iteration 142 - loss value [[681.62783793]] accuracy 0.677\n",
            "Iteration 143 - loss value [[680.97638312]] accuracy 0.601\n",
            "Iteration 144 - loss value [[679.49567899]] accuracy 0.663\n",
            "Iteration 145 - loss value [[664.60120828]] accuracy 0.818\n",
            "Iteration 146 - loss value [[697.97739669]] accuracy 0.58\n",
            "Iteration 147 - loss value [[681.08653776]] accuracy 0.642\n",
            "Iteration 148 - loss value [[665.06165774]] accuracy 0.71\n",
            "Iteration 149 - loss value [[683.6170298]] accuracy 0.614\n"
          ]
        }
      ]
    }
  ]
}